import json
from datetime import datetime
import os
import requests


base_dir = os.path.dirname(os.path.abspath(__file__))
json_file_path = os.path.join(base_dir, "offres_analyste.json")

with open(json_file_path, "r", encoding="utf-8") as f:
    data= json.load(f)

#competence > libelle

offres = data if isinstance(data, list) else [data]

#competence_unique = set()
#for offre in offres:
#    for item in offre.get("competences", []):
#        lib = item.get("libelle")
#       if lib:
#            competence_unique.add(lib)

#print(competence_unique)
#print(len(competence_unique))

#updated_path = os.path.join(base_dir, "competence_un.json")
#with open(updated_path, "w", encoding="utf-8") as f:
 #   json.dump(list(competence_unique), f, indent=2, ensure_ascii=False)



import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import re
from collections import Counter

# DICTIONNAIRES
OUTILS_TECH = {
    'Python': ['python', 'pandas', 'numpy', 'matplotlib', 'seaborn', 'jupyter'],
    'R': ['langage r', ' r ', 'rstudio', 'ggplot', 'dplyr', 'tidyverse'],
    'SQL': ['sql', 'mysql', 'postgresql', 'postgres', 'oracle', 't-sql', 'tsql', 'pl/sql', 'plsql', 'requ√™te', 'requ√™tes', 'base de donn√©es', 'bases de donn√©es', 'sgbd'],
    'SAS': ['sas'],
    'VBA': ['vba', 'macro', 'macros excel'],
    'Power BI': ['power bi', 'powerbi', 'power-bi', 'dax'],
    'Tableau': ['tableau software', 'tableau desktop'],
    'Excel': ['excel', 'tableur', 'tableaux crois√©s', 'tcd', 'graphiques excel'],
    'Qlik': ['qlik', 'qlikview', 'qlik sense'],
    'Looker': ['looker', 'looker studio', 'data studio'],
    'AWS': ['aws', 'amazon web', 's3', 'redshift', 'athena'],
    'Azure': ['azure', 'synapse', 'azure data'],
    'GCP': ['google cloud', 'bigquery', 'gcp'],
    'Databricks': ['databricks'],
    'Snowflake': ['snowflake'],
    'Spark': ['spark', 'pyspark', 'apache spark'],
    'Talend': ['talend'],
    'Informatica': ['informatica'],
    'SSIS': ['ssis', 'integration services'],
    'Airflow': ['airflow'],
    'SPSS': ['spss'],
    'Stata': ['stata'],
    'MongoDB': ['mongodb', 'mongo'],
    'Elasticsearch': ['elasticsearch', 'elastic'],
    'Git': ['git', 'github', 'gitlab', 'versionning'],
    'SAP': ['sap'],
    'Salesforce': ['salesforce'],
    'Google Analytics': ['google analytics', 'ga4', 'analytics'],
}

CATEGORIES_THEMATIQUES = {
    'Analyse de donn√©es': ['analys', 'traiter', 'interpr√©ter', 'exploiter', '√©tudier', 'donn√©es', 'data', 'statistique', 'quantitatif'],
    'Bases de donn√©es': ['base', 'bdd', 'sgbd', 'requ√™te', 'sql', 'mongodb', 'oracle'],
    'Visualisation': ['visualis', 'dashboard', 'tableau de bord', 'graphique', 'rapport', 'reporting', 'dataviz', 'bi', 'business intelligence'],
    'Mod√©lisation statistique': ['mod√®le', 'mod√©lis', 'r√©gression', 'pr√©vision', 'pr√©dictif', 'algorithme', 'machine learning', 'statistique'],
    'Qualit√© & conformit√©': ['qualit√©', 'conformit√©', 'contr√¥le', 'audit', 'norme', 'iso', 'haccp', 'qse', 'proc√©dure'],
    'Gestion de projet': ['projet', 'planif', 'coordin', 'pilotage', 'suivi', 'organisation', 'gestion de projet'],
    'Finance & comptabilit√©': ['financ', 'budget', 'compta', 'co√ªt', 'prix', 'investissement', 'rentabilit√©', 'tr√©sorerie'],
    'Communication & pr√©sentation': ['communic', 'pr√©sent', 'r√©daction', 'r√©diger', 'synth√®s', 'rapport', 'pr√©sentation', 'oral'],
    'S√©curit√© IT': ['s√©curit√©', 'cyber', 'protection', 'sauvegarde', 'confidentialit√©'],
    'ETL & Data Engineering': ['etl', 'int√©gration', 'pipeline', 'extraction', 'transformation', 'chargement', 'flux de donn√©es'],
}

# FONCTIONS
def detecter_outils_ameliore(competence, outils_dict):
    comp_lower = competence.lower()
    outils_detectes = set()
    for outil, patterns in outils_dict.items():
        for pattern in patterns:
            if pattern.lower() in comp_lower:
                outils_detectes.add(outil)
                break
    return list(outils_detectes)

def categoriser_thematique(competence, categories):
    comp_lower = competence.lower()
    categories_trouvees = []
    for categorie, keywords in categories.items():
        for keyword in keywords:
            if keyword in comp_lower:
                categories_trouvees.append(categorie)
                break
    return categories_trouvees if categories_trouvees else ['Autre']



# CHARGEMENT
with open('offres_analyste.json', 'r', encoding='utf-8') as f:
    offres = json.load(f)

print(f"\nNombre d'offres d'emploi : {len(offres)}")


# === CHARGEMENT (existant) ===
with open('offres_analyste.json', 'r', encoding='utf-8') as f:
    offres = json.load(f)

print(f"\nNombre d'offres brutes : {len(offres)}")


# Filtrage des offres Data Analyst (exclure analystes qualit√©/labo/cyber)
keywords_data = [
    'data', 'donn√©es', 'business intelligence', 'bi', 'reporting', 
    'dashboard', 'tableau de bord', 'sql', 'python', 'power bi', 'powerbi',
    'analyse de donn√©es', 'data analyst', 'analyste donn√©es', 'analyste data',
    'datawarehouse', 'etl', 'base de donn√©es', 'bases de donn√©es',
    'visualisation', 'dataviz', 'kpi', 'indicateurs', 'excel'
]

keywords_exclusion = [
    'laboratoire', '√©chantillon', 'pr√©l√®vement', 'contr√¥le qualit√©',
    'chimie', 'biologie', 'pharmaceutique', 'analyse m√©dicale',
    'soc', 'cybers√©curit√©', 'siem', 'incident', 'menace',
    'cr√©dit', 'risque financier', 'conformit√© bancaire',
    '√©talonnage', 'mat√©riaux', 'essais', 'norme iso',
    'toxicologie', '√©chantillonnage', 'chromatographie'
]

offres_filtrees = []

for offre in offres:
    texte = offre.get('intitule', '').lower()
    if 'competences' in offre and offre['competences']:
        texte += ' ' + ' '.join([c.get('libelle', '').lower() for c in offre['competences']])
    
    # Exclure si contient keywords d'exclusion
    if any(kw in texte for kw in keywords_exclusion):
        continue
    
    # Garder si contient keywords data analyst
    if any(kw in texte for kw in keywords_data):
        offres_filtrees.append(offre)

print(f"\nOffres filtr√©es : {len(offres_filtrees)}/{len(offres)} ({len(offres_filtrees)/len(offres)*100:.1f}% du dataset)")

# Utiliser le dataset filtr√© pour l'analyse
offres = offres_filtrees

# Extraire comp√©tences
competences_brutes = []
for offre in offres:
    if 'competences' in offre and offre['competences']:
        for comp in offre['competences']:
            if 'libelle' in comp:
                competences_brutes.append(comp['libelle'])

print(f"Total de comp√©tences (avec r√©p√©titions) : {len(competences_brutes)}")

competences_count = Counter(competences_brutes)
competences_uniques = list(competences_count.keys())

print(f"Comp√©tences uniques : {len(competences_uniques)}")


# PHASE 1 : OUTILS/LANGAGES


competences_analysees = {}
all_outils_avec_freq = []

for comp_unique in competences_uniques:
    freq = competences_count[comp_unique]
    outils = detecter_outils_ameliore(comp_unique, OUTILS_TECH)
    themes = categoriser_thematique(comp_unique, CATEGORIES_THEMATIQUES)
    
    competences_analysees[comp_unique] = {
        'frequence': freq,
        'outils': outils,
        'themes': themes
    }
    
    for outil in outils:
        all_outils_avec_freq.extend([outil] * freq)

outils_count = Counter(all_outils_avec_freq)

print(f"\nTotal mentions d'outils : {len(all_outils_avec_freq)}")

print("\n TOP 20 OUTILS/LANGAGES DATA ANALYST")
print("-" * 80)
print("(% = pourcentage d'offres demandant cet outil)\n")
for i, (outil, count) in enumerate(outils_count.most_common(20), 1):
    pct = (count / len(offres)) * 100
    bar = '‚ñà' * min(int(pct * 2), 50)
    print(f"{i:2d}. {outil:20s} ‚îÇ{bar} {count:4d} offres ({pct:5.1f}%)")


# PHASE 2 : TH√âMATIQUES


print("PHASE 2 : R√âPARTITION PAR TH√âMATIQUE")


all_themes_avec_freq = []
for comp_unique, data in competences_analysees.items():
    for theme in data['themes']:
        all_themes_avec_freq.extend([theme] * data['frequence'])

themes_count = Counter(all_themes_avec_freq)

print("\n R√âPARTITION DES COMP√âTENCES PAR TH√âMATIQUE")
print("-" * 80)
for theme, count in themes_count.most_common():
    pct = (count / len(competences_brutes)) * 100
    print(f"{theme:35s} : {count:4d} mentions ({pct:5.1f}%)")

# PHASE 3 : CLUSTERING

print("PHASE 3 : CLUSTERING DES COMP√âTENCES")

comp_avec_outils = {k: v for k, v in competences_analysees.items() if v['outils']}
comp_sans_outils = {k: v for k, v in competences_analysees.items() if not v['outils']}

print(f"\nComp√©tences uniques avec outils : {len(comp_avec_outils)}")
print(f"Comp√©tences uniques sans outils : {len(comp_sans_outils)}")

if len(comp_sans_outils) > 10:
    textes = list(comp_sans_outils.keys())
    
    vectorizer = TfidfVectorizer(max_features=150, ngram_range=(1, 2), min_df=2)
    tfidf = vectorizer.fit_transform(textes)
    
    n_clusters = min(15, len(comp_sans_outils) // 10)
    print(f"Nombre de clusters : {n_clusters}")
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(tfidf)
    
    cluster_names = {}
    for cluster_id in range(n_clusters):
        comps_cluster = [textes[i] for i in range(len(textes)) if labels[i] == cluster_id]
        
        mots_cluster = []
        for comp in comps_cluster:
            mots = re.findall(r'\b\w{5,}\b', comp.lower())
            mots_cluster.extend(mots)
        
        stop_words = {'dans', 'pour', 'avec', '√™tre', 'avoir', 'faire', 'mettre', 'donner'}
        mots_filtres = [m for m in mots_cluster if m not in stop_words]
        
        top_mots = Counter(mots_filtres).most_common(2)
        cluster_names[cluster_id] = ' / '.join([m.capitalize() for m, _ in top_mots])
    
    for i, comp in enumerate(textes):
        comp_sans_outils[comp]['cluster'] = cluster_names[labels[i]]
else:
    for comp in comp_sans_outils:
        comp_sans_outils[comp]['cluster'] = 'G√©n√©ral'

for comp, data in comp_avec_outils.items():
    data['cluster'] = ', '.join(data['outils'])


# STATISTIQUES FINALES

print("STATISTIQUES FINALES")

cluster_freq = Counter()
for comp, data in competences_analysees.items():
    cluster = data.get('cluster', 'Non class√©')
    cluster_freq[cluster] += data['frequence']

print("\n TOP 20 CLUSTERS PAR FR√âQUENCE D'APPARITION")
print("-" * 80)
print("(= nombre total d'offres mentionnant ce cluster)\n")
for i, (cluster, count) in enumerate(cluster_freq.most_common(20), 1):
    pct = (count / len(offres)) * 100
    print(f"{i:2d}. {cluster:45s} : {count:4d} ({pct:5.1f}%)")

# RAPPORT D√âTAILL√â


print("\nClassement par nombre d'offres :\n")
for i, (outil, count) in enumerate(outils_count.most_common(), 1):
    pct = (count / len(offres)) * 100
    print(f"{i:2d}. {outil:25s} : {count:4d} offres ({pct:5.1f}%)")

print("\n" + "="*80)
print("SECTION 2 : TOP 20 COMP√âTENCES LES PLUS DEMAND√âES")
print("="*80)
print()
for i, (comp, count) in enumerate(competences_count.most_common(20), 1):
    pct = (count / len(offres)) * 100
    print(f"{i:2d}. {comp:65s} : {count:3d} ({pct:4.1f}%)")

print("\n" + "="*80)
print("SECTION 3 : D√âTAIL DES CLUSTERS (TOP 5)")
print("="*80)

for i, (cluster, count) in enumerate(cluster_freq.most_common(5), 1):
    pct = (count / len(offres)) * 100
    print(f"\n{'='*80}")
    print(f"üîπ CLUSTER #{i}: {cluster}")
    print(f"   Fr√©quence : {count} mentions ({pct:.1f}% des offres)")
    print(f"{'='*80}")
    
    # Trouver les comp√©tences de ce cluster
    comps_du_cluster = [
        (comp, data['frequence']) 
        for comp, data in competences_analysees.items() 
        if data.get('cluster') == cluster
    ]
    comps_du_cluster.sort(key=lambda x: x[1], reverse=True)
    
    print("\nTop 5 comp√©tences de ce cluster :")
    for j, (comp, freq) in enumerate(comps_du_cluster[:5], 1):
        pct_comp = (freq / len(offres)) * 100
        print(f"  {j:2d}. {comp:60s} : {freq:3d} ({pct_comp:4.1f}%)")
    
    if len(comps_du_cluster) > 5:
        print(f"  ... et {len(comps_du_cluster) - 5} autres comp√©tences")

print("\n" + "="*80)
print("R√âSUM√â FINAL")
print("="*80)
print(f"\n Statistiques globales :")
print(f"  ‚Ä¢ Offres analys√©es : {len(offres)}")
print(f"  ‚Ä¢ Comp√©tences totales (avec r√©p√©titions) : {len(competences_brutes)}")
print(f"  ‚Ä¢ Comp√©tences uniques : {len(competences_uniques)}")
print(f"  ‚Ä¢ Outils/langages d√©tect√©s : {len(outils_count)}")
print(f"  ‚Ä¢ Th√©matiques identifi√©es : {len(themes_count)}")
print(f"  ‚Ä¢ Clusters cr√©√©s : {len(cluster_freq)}")
print("\n" + "="*80)



#Plage des dates de cr√©ation des offres
date_strings = [offre.get("dateCreation") for offre in offres if offre.get("dateCreation")]
if date_strings:
    dates = [datetime.fromisoformat(s.replace("Z", "+00:00")) for s in date_strings]
    print('date de cr√©ation des offfres minimum:', min(dates), 'et maximum:', max(dates))
    print("="*80)





# EXPLORATION DES NIVEAUX D'EXP√âRIENCE
print("EXPLORATION : NIVEAUX D'EXP√âRIENCE DISPONIBLES")
print("="*80)

niveaux_bruts = []
for offre in offres:
    niveau = offre.get('experienceLibelle')
    if niveau:
        niveaux_bruts.append(niveau)

niveaux_count = Counter(niveaux_bruts)

print(f"\nTotal offres avec niveau sp√©cifi√© : {len(niveaux_bruts)} / {len(offres)}")
print("\nR√©partition :")
for niveau, count in niveaux_count.most_common():
    pct = (count / len(offres)) * 100
    print(f"  {niveau:40s} : {count:4d} offres ({pct:5.1f}%)")





comps_par_niveau = {
    'Junior': [],
    'Confirm√©': [],
    'Senior': []
}

for offre in offres:
    niveau_raw = offre.get('experienceLibelle', '')
    niveau = 'Junior' if 'junior' in niveau_raw.lower() or '0-2' in niveau_raw else \
             'Senior' if 'senior' in niveau_raw.lower() or '6+' in niveau_raw else \
             'Confirm√©'
    
    if 'competences' in offre and offre['competences']:
        for comp in offre['competences']:
            if 'libelle' in comp:
                comps_par_niveau[niveau].append(comp['libelle'])


for niveau in ['Junior', 'Confirm√©', 'Senior']:
    if not comps_par_niveau[niveau]:
        continue
    
    # Compter les mots
    mots_niveau = []
    for comp in comps_par_niveau[niveau]:
        mots = re.findall(r'\b\w{5,}\b', comp.lower())
        mots_niveau.extend(mots)
    
    mots_count = Counter(mots_niveau)
    
    # Stop words
    stop = {'dans', 'pour', 'avec', '√™tre', 'avoir', 'faire', 'mettre', 
            'cette', 'tous', 'plus', 'leurs', 'dont'}
    
   # Analyse simplifi√©e : d√©butant vs exp√©riment√©
print("\n" + "="*80)
print("D√âBUTANT VS EXP√âRIMENT√â")
print("="*80)

# Regrouper en 2 cat√©gories seulement
debutant = []
experimente = []

for offre in offres:
    niveau = offre.get('experienceLibelle', '').lower()
    
    if 'competences' in offre and offre['competences']:
        comps = [c.get('libelle', '') for c in offre['competences']]
        
        # D√©butant = 0-2 ans
        if any(x in niveau for x in ['d√©butant', '0', '1 an', '2 an', '6 mois']):
            debutant.extend(comps)
        # Exp√©riment√© = 3+ ans
        elif any(x in niveau for x in ['3 an', '4 an', '5 an', '6 an', '7 an', '8 an', '9 an', '10 an']):
            experimente.extend(comps)

print(f"\nOffres d√©butant : {len([o for o in offres if any(x in o.get('experienceLibelle', '').lower() for x in ['d√©butant', '0', '1 an', '2 an'])])}")
print(f"Offres exp√©riment√© : {len([o for o in offres if any(x in o.get('experienceLibelle', '').lower() for x in ['3 an', '4 an', '5 an', '6 an'])])}")

# Mots cl√©s par cat√©gorie
mots_debutant = []
mots_exp = []

for comp in debutant:
    mots = re.findall(r'\b\w{5,}\b', comp.lower())
    mots_debutant.extend(mots)

for comp in experimente:
    mots = re.findall(r'\b\w{5,}\b', comp.lower())
    mots_exp.extend(mots)

stop = {'dans', 'pour', 'avec', '√™tre', 'avoir', 'faire', 'mettre', 'donner',
        'tous', 'plus', 'leurs', 'cette', 'dont', 'savoir', 'pouvoir'}

count_deb = Counter([m for m in mots_debutant if m not in stop])
count_exp = Counter([m for m in mots_exp if m not in stop])

print("\nTop 10 mots - Postes D√âBUTANT :")
for i, (mot, count) in enumerate(count_deb.most_common(10), 1):
    print(f"  {i:2d}. {mot:20s} : {count:3d}")

print("\nTop 10 mots - Postes EXP√âRIMENT√â :")
for i, (mot, count) in enumerate(count_exp.most_common(10), 1):
    print(f"  {i:2d}. {mot:20s} : {count:3d}")

# Identifier diff√©rences
print("\nMots sur-repr√©sent√©s chez les EXP√âRIMENT√âS :")
for mot in count_exp.most_common(20):
    freq_deb = count_deb.get(mot[0], 0) / len(mots_debutant) * 100 if mots_debutant else 0
    freq_exp = count_exp.get(mot[0], 0) / len(mots_exp) * 100 if mots_exp else 0
    
    if freq_exp > freq_deb * 1.5 and freq_exp > 3:  # 50% plus fr√©quent ET >3%
        print(f"  ‚Ä¢ {mot[0]:20s} : {freq_deb:4.1f}% (d√©b) ‚Üí {freq_exp:4.1f}% (exp)")


# Domaines d'expertise
print("\n" + "="*80)
print("DOMAINES D'EXPERTISE RECHERCH√âS")
print("="*80)

domaines = {
    'Finance/Gestion': ['financ', 'budget', 'compta', 'tr√©sor', 'cr√©dit', 'risque'],
    'Donn√©es/BI': ['donn√©es', 'data', 'business intelligence', 'reporting', 'dashboard'],
    'Statistiques': ['statistique', 'mod√®le', 'pr√©vision', 'algorithme'],
    'March√©/Concurrence': ['march√©', 'concurren', 'veille', 'strat√©gi'],
    'Performance': ['performance', 'indicateurs', 'kpi', 'optimis'],
}

domaines_count = Counter()
for comp in competences_brutes:
    comp_lower = comp.lower()
    for domaine, kws in domaines.items():
        if any(kw in comp_lower for kw in kws):
            domaines_count[domaine] += 1

print("\nDomaines d'expertise par fr√©quence :")
for i, (dom, count) in enumerate(domaines_count.most_common(), 1):
    pct = (count / len(offres)) * 100
    print(f"{i}. {dom:25s} : {count:3d} mentions ({pct:5.1f}%)")




